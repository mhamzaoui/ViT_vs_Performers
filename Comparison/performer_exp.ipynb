{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TerozXs-CQsP",
        "outputId": "3edd044b-e211-4ed1-e584-73a2cf686b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting performer-pytorch\n",
            "  Downloading performer_pytorch-1.1.4-py3-none-any.whl.metadata (763 bytes)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from performer-pytorch) (0.8.0)\n",
            "Collecting local-attention>=1.1.1 (from performer-pytorch)\n",
            "  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from performer-pytorch) (2.5.1+cu121)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from performer-pytorch)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->performer-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->performer-pytorch) (3.0.2)\n",
            "Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
            "Downloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2887 sha256=0526021ccd041d0e8ec84eaad899f8c2caefabedd0c30303839019a6a366712e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 local-attention-1.9.15 performer-pytorch-1.1.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install performer-pytorch\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBU9jxGPC0vH",
        "outputId": "741af641-526a-4625-fc40-1b151baba779"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Training: 100%|██████████| 469/469 [19:59<00:00,  2.56s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.9722, Train Acc: 0.2536\n",
            "Val Loss: 1.7660, Val Acc: 0.3025\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 Training: 100%|██████████| 469/469 [20:06<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2\n",
            "Train Loss: 1.4407, Train Acc: 0.4632\n",
            "Val Loss: 1.0084, Val Acc: 0.6402\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 Training: 100%|██████████| 469/469 [20:07<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\n",
            "Train Loss: 0.8738, Train Acc: 0.6902\n",
            "Val Loss: 0.7575, Val Acc: 0.7262\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 Training: 100%|██████████| 469/469 [20:08<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4\n",
            "Train Loss: 0.6615, Train Acc: 0.7702\n",
            "Val Loss: 0.5298, Val Acc: 0.8235\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 Training: 100%|██████████| 469/469 [20:08<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5\n",
            "Train Loss: 0.4927, Train Acc: 0.8414\n",
            "Val Loss: 0.4200, Val Acc: 0.8712\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6 Training: 100%|██████████| 469/469 [20:10<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6\n",
            "Train Loss: 0.4049, Train Acc: 0.8759\n",
            "Val Loss: 0.3351, Val Acc: 0.9001\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7 Training: 100%|██████████| 469/469 [20:09<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:53<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7\n",
            "Train Loss: 0.3431, Train Acc: 0.8982\n",
            "Val Loss: 0.2930, Val Acc: 0.9157\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8 Training: 100%|██████████| 469/469 [20:09<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8\n",
            "Train Loss: 0.3024, Train Acc: 0.9103\n",
            "Val Loss: 0.2970, Val Acc: 0.9100\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9 Training: 100%|██████████| 469/469 [20:04<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9\n",
            "Train Loss: 0.2697, Train Acc: 0.9199\n",
            "Val Loss: 0.2505, Val Acc: 0.9260\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10 Training:  28%|██▊       | 129/469 [05:30<14:29,  2.56s/it]"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from performers_pytorch import PerformerLM\n",
        "from autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "# Define the exponential kernel function\n",
        "def performer_exponential_kernel(data, is_query=True, normalize=False, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Exponential kernel for Performer attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        data: Input tensor\n",
        "        is_query: Boolean indicating if input is query (True) or key (False)\n",
        "        normalize: Whether to normalize the output\n",
        "        eps: Small constant for numerical stability\n",
        "    \"\"\"\n",
        "    data_norm = torch.norm(data, p=2, dim=-1, keepdim=True)\n",
        "    data_normalized = data / (data_norm + eps)\n",
        "\n",
        "    if normalize:\n",
        "        return data_normalized\n",
        "\n",
        "    return torch.exp(-data_norm) * data_normalized if is_query else torch.exp(data_norm) * data_normalized\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load and preprocess MNIST\n",
        "data = np.load('mnist.npz')\n",
        "x_train = torch.from_numpy(data['x_train']).float()\n",
        "y_train = torch.from_numpy(data['y_train']).long()\n",
        "x_test = torch.from_numpy(data['x_test']).float()\n",
        "y_test = torch.from_numpy(data['y_test']).long()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train.view(-1, 784)  # Flatten 28x28 to 784\n",
        "x_test = x_test.view(-1, 784)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Initialize model\n",
        "model = PerformerLM(\n",
        "    num_tokens=256,  # Number of unique tokens (pixel values)\n",
        "    dim=256,\n",
        "    depth=4,\n",
        "    max_seq_len=784,  # MNIST flattened size\n",
        "    heads=4,\n",
        "    causal=False,\n",
        "    reversible=True,\n",
        "    use_scalenorm=True,\n",
        "    generalized_attention=True,\n",
        "    kernel_fn=performer_exponential_kernel,  # Use the defined kernel function\n",
        "    local_attn_heads=(4, 4, 2, 2),\n",
        "    no_projection=True  # Disable projection if not needed\n",
        ").to(DEVICE)\n",
        "\n",
        "# Add classification head\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(256, 10),  # 10 classes for MNIST\n",
        "    nn.LogSoftmax(dim=1)\n",
        ").to(DEVICE)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):  # Start from 0\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, (data, target) in enumerate(tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1} Training')):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        # Convert to indices for Performer\n",
        "        data = (data * 255).clamp(0, 255).long()  # Ensure data is in the correct range\n",
        "\n",
        "        # Forward pass\n",
        "        features = model(data)\n",
        "        features = features.mean(dim=1)  # Global average pooling\n",
        "        output = classifier(features)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        pred = output.argmax(dim=1)\n",
        "        train_correct += pred.eq(target).sum().item()\n",
        "        train_total += target.size(0)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm.tqdm(val_loader, desc='Validation'):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            data = (data * 255).clamp(0, 255).long()  # Ensure data is in the correct range\n",
        "\n",
        "            features = model(data)\n",
        "            features = features.mean(dim=1)\n",
        "            output = classifier(features)\n",
        "\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            val_correct += pred.eq(target).sum().item()\n",
        "            val_total += target.size(0)\n",
        "\n",
        "    # Print metrics\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n')"
      ]
    },
  

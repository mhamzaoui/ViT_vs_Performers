{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TerozXs-CQsP",
        "outputId": "3edd044b-e211-4ed1-e584-73a2cf686b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting performer-pytorch\n",
            "  Downloading performer_pytorch-1.1.4-py3-none-any.whl.metadata (763 bytes)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from performer-pytorch) (0.8.0)\n",
            "Collecting local-attention>=1.1.1 (from performer-pytorch)\n",
            "  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from performer-pytorch) (2.5.1+cu121)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from performer-pytorch)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->performer-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->performer-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->performer-pytorch) (3.0.2)\n",
            "Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
            "Downloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2887 sha256=0526021ccd041d0e8ec84eaad899f8c2caefabedd0c30303839019a6a366712e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 local-attention-1.9.15 performer-pytorch-1.1.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install performer-pytorch\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def performer_exponential_kernel(data, is_query=True, normalize=False, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Exponential kernel for Performer attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        data: Input tensor\n",
        "        is_query: Boolean indicating if input is query (True) or key (False)\n",
        "        normalize: Whether to normalize the output\n",
        "        eps: Small constant for numerical stability\n",
        "    \"\"\"\n",
        "    data_norm = torch.norm(data, p=2, dim=-1, keepdim=True)\n",
        "    data_normalized = data / (data_norm + eps)\n",
        "\n",
        "    if normalize:\n",
        "        return data_normalized\n",
        "\n",
        "    return torch.exp(-data_norm) * data_normalized if is_query else torch.exp(data_norm) * data_normalized"
      ],
      "metadata": {
        "id": "R9XmJwzdCySe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from performers_pytorch import PerformerLM\n",
        "from autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "# Define the exponential kernel function\n",
        "def performer_exponential_kernel(data, is_query=True, normalize=False, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Exponential kernel for Performer attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        data: Input tensor\n",
        "        is_query: Boolean indicating if input is query (True) or key (False)\n",
        "        normalize: Whether to normalize the output\n",
        "        eps: Small constant for numerical stability\n",
        "    \"\"\"\n",
        "    data_norm = torch.norm(data, p=2, dim=-1, keepdim=True)\n",
        "    data_normalized = data / (data_norm + eps)\n",
        "\n",
        "    if normalize:\n",
        "        return data_normalized\n",
        "\n",
        "    return torch.exp(-data_norm) * data_normalized if is_query else torch.exp(data_norm) * data_normalized\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load and preprocess MNIST\n",
        "data = np.load('mnist.npz')\n",
        "x_train = torch.from_numpy(data['x_train']).float()\n",
        "y_train = torch.from_numpy(data['y_train']).long()\n",
        "x_test = torch.from_numpy(data['x_test']).float()\n",
        "y_test = torch.from_numpy(data['y_test']).long()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train.view(-1, 784)  # Flatten 28x28 to 784\n",
        "x_test = x_test.view(-1, 784)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Initialize model\n",
        "model = PerformerLM(\n",
        "    num_tokens=256,  # Number of unique tokens (pixel values)\n",
        "    dim=256,\n",
        "    depth=4,\n",
        "    max_seq_len=784,  # MNIST flattened size\n",
        "    heads=4,\n",
        "    causal=False,\n",
        "    reversible=True,\n",
        "    use_scalenorm=True,\n",
        "    generalized_attention=True,\n",
        "    kernel_fn=performer_exponential_kernel,  # Use the defined kernel function\n",
        "    local_attn_heads=(4, 4, 2, 2),\n",
        "    no_projection=True  # Disable projection if not needed\n",
        ").to(DEVICE)\n",
        "\n",
        "# Add classification head\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(256, 10),  # 10 classes for MNIST\n",
        "    nn.LogSoftmax(dim=1)\n",
        ").to(DEVICE)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):  # Start from 0\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, (data, target) in enumerate(tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1} Training')):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        # Convert to indices for Performer\n",
        "        data = (data * 255).clamp(0, 255).long()  # Ensure data is in the correct range\n",
        "\n",
        "        # Forward pass\n",
        "        features = model(data)\n",
        "        features = features.mean(dim=1)  # Global average pooling\n",
        "        output = classifier(features)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        pred = output.argmax(dim=1)\n",
        "        train_correct += pred.eq(target).sum().item()\n",
        "        train_total += target.size(0)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm.tqdm(val_loader, desc='Validation'):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            data = (data * 255).clamp(0, 255).long()  # Ensure data is in the correct range\n",
        "\n",
        "            features = model(data)\n",
        "            features = features.mean(dim=1)\n",
        "            output = classifier(features)\n",
        "\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            val_correct += pred.eq(target).sum().item()\n",
        "            val_total += target.size(0)\n",
        "\n",
        "    # Print metrics\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBU9jxGPC0vH",
        "outputId": "741af641-526a-4625-fc40-1b151baba779"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Training: 100%|██████████| 469/469 [19:59<00:00,  2.56s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.9722, Train Acc: 0.2536\n",
            "Val Loss: 1.7660, Val Acc: 0.3025\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 Training: 100%|██████████| 469/469 [20:06<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2\n",
            "Train Loss: 1.4407, Train Acc: 0.4632\n",
            "Val Loss: 1.0084, Val Acc: 0.6402\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 Training: 100%|██████████| 469/469 [20:07<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\n",
            "Train Loss: 0.8738, Train Acc: 0.6902\n",
            "Val Loss: 0.7575, Val Acc: 0.7262\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 Training: 100%|██████████| 469/469 [20:08<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4\n",
            "Train Loss: 0.6615, Train Acc: 0.7702\n",
            "Val Loss: 0.5298, Val Acc: 0.8235\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 Training: 100%|██████████| 469/469 [20:08<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5\n",
            "Train Loss: 0.4927, Train Acc: 0.8414\n",
            "Val Loss: 0.4200, Val Acc: 0.8712\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6 Training: 100%|██████████| 469/469 [20:10<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6\n",
            "Train Loss: 0.4049, Train Acc: 0.8759\n",
            "Val Loss: 0.3351, Val Acc: 0.9001\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Training: 100%|██████████| 469/469 [20:09<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:53<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7\n",
            "Train Loss: 0.3431, Train Acc: 0.8982\n",
            "Val Loss: 0.2930, Val Acc: 0.9157\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Training: 100%|██████████| 469/469 [20:09<00:00,  2.58s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8\n",
            "Train Loss: 0.3024, Train Acc: 0.9103\n",
            "Val Loss: 0.2970, Val Acc: 0.9100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Training: 100%|██████████| 469/469 [20:04<00:00,  2.57s/it]\n",
            "Validation: 100%|██████████| 79/79 [00:52<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9\n",
            "Train Loss: 0.2697, Train Acc: 0.9199\n",
            "Val Loss: 0.2505, Val Acc: 0.9260\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Training:  28%|██▊       | 129/469 [05:30<14:29,  2.56s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L_loss = [1.9722, 1.4407, 0.8738, 0.6615, 0.4927, 0.4049, 0.3431, 0.3024, 0.2697, 0.2431]\n",
        "L_accuracy = [0.2536, 0.4632, 0.6902, 0.7702, 0.8414, 0.8759, 0.8982, 0.9103, 0.9199, 0.9267]\n",
        "L_val_loss = [1.7660, 1.0084, 0.7575, 0.5298, 0.4200, 0.3351, 0.2930, 0.2970, 0.2505, 0.2628]\n",
        "L_val_accuracy = [0.3025, 0.6402, 0.7262, 0.8235, 0.8712, 0.9001, 0.9157, 0.9100, 0.9260, 0.9394]"
      ],
      "metadata": {
        "id": "85W99Q9aUSSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRBKI08qURFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have access to the FastAttention class\n",
        "from performers_pytorch import FastAttention\n",
        "\n",
        "# Create an instance of FastAttention for debugging\n",
        "fast_attention_instance = FastAttention(\n",
        "    dim_heads=4,\n",
        "    nb_features=128,  # Adjust as necessary\n",
        "    ortho_scaling=0,\n",
        "    causal=False,\n",
        "    generalized_attention=False,\n",
        "    kernel_fn=performer_exponential_kernel,\n",
        "    no_projection=True\n",
        ")\n",
        "\n",
        "# Print available methods and attributes\n",
        "print(\"Available methods and attributes in FastAttention:\")\n",
        "print(dir(fast_attention_instance))\n",
        "\n",
        "# Check specifically for the redraw_projection_matrix method\n",
        "if 'redraw_projection_matrix' in dir(fast_attention_instance):\n",
        "    print(\"redraw_projection_matrix method is available.\")\n",
        "else:\n",
        "    print(\"redraw_projection_matrix method is NOT available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lk6M7f0wztG",
        "outputId": "5fa53e9b-c4d2-486f-97e5-238763399fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available methods and attributes in FastAttention:\n",
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'causal', 'children', 'compile', 'cpu', 'create_projection', 'cuda', 'dim_heads', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'generalized_attention', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'kernel_fn', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'nb_features', 'no_projection', 'ortho_scaling', 'parameters', 'projection_matrix', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "redraw_projection_matrix method is NOT available.\n"
          ]
        }
      ]
    }
  ]
}
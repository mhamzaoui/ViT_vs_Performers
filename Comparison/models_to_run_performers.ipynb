{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: local_attention in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (1.9.15)\n",
      "Requirement already satisfied: einops>=0.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from local_attention) (0.8.0)\n",
      "Requirement already satisfied: torch in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from local_attention) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch->local_attention) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch->local_attention) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install local_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: axial_positional_embedding in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from axial_positional_embedding) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch->axial_positional_embedding) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch->axial_positional_embedding) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install axial_positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\Users\\\\Guillaume\\\\ViT_vs_Performers\\\\src\\\\Performers')\n",
    "from performers_pytorch import PerformerLM\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'y_train', 'y_test']\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' y_test = y_test.unsqueeze(1) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "################################################################## MNIST #################################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "os.chdir('c:\\\\Users\\\\Guillaume\\\\ViT_vs_Performers\\\\Data')\n",
    "data = np.load('mnist.npz')\n",
    "print(data.files)\n",
    "print(data['x_train'][0])\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "## Changing to pytorch tensors\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "## Adding the channels dimension\n",
    "\n",
    "x_train = x_train.unsqueeze(1)\n",
    "\"\"\" y_train = y_train.unsqueeze(1) \"\"\"\n",
    "x_test = x_test.unsqueeze(1)\n",
    "\"\"\" y_test = y_test.unsqueeze(1) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillaume\\AppData\\Local\\Temp\\ipykernel_20864\\2895174009.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Guillaume\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "training:   0%|          | 0/100000 [00:00<?, ?it/s]C:\\Users\\Guillaume\\AppData\\Local\\Temp\\ipykernel_20864\\2895174009.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\Guillaume\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "training:   0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GRADIENT_ACCUMULATE_EVERY):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 84\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28mnext\u001b[39m(\u001b[43mtrain_loader\u001b[49m), return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 2048\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "model = PerformerLM(\n",
    "    num_tokens = 256,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 8,\n",
    "    causal = True,\n",
    "    reversible = True,\n",
    "    nb_features = 256,\n",
    "    use_scalenorm = True,\n",
    "    shift_tokens = True,\n",
    "    local_attn_heads = (8, 8, 8, 6, 4, 2)\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "## model.cuda()\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "#with gzip.open('./data/enwik8.gz') as file:\n",
    "#    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
    "#    trX, vaX = np.split(X, [int(90e6)])\n",
    "#    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq #.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "#train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "#val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "#train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "#val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        with autocast():\n",
    "            loss = model(next(train_loader), return_loss = True)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "\n",
    "    scaler.unscale_(optim)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0 and i != 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

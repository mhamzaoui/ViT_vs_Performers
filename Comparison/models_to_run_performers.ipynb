{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting local_attention\n",
      "  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\n",
      "Requirement already satisfied: einops>=0.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from local_attention) (0.8.0)\n",
      "Requirement already satisfied: torch in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from local_attention) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->local_attention) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch->local_attention) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch->local_attention) (3.0.2)\n",
      "Downloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\n",
      "Installing collected packages: local_attention\n",
      "Successfully installed local_attention-1.9.15\n"
     ]
    }
   ],
   "source": [
    "!pip install local_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting axial_positional_embedding\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from axial_positional_embedding) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from torch->axial_positional_embedding) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch->axial_positional_embedding) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guillaume\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch->axial_positional_embedding) (3.0.2)\n",
      "Building wheels for collected packages: axial_positional_embedding\n",
      "  Building wheel for axial_positional_embedding (pyproject.toml): started\n",
      "  Building wheel for axial_positional_embedding (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for axial_positional_embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2894 sha256=9dbe8b8d62021f87f673c19b9ba7e5b4fb4a696b563706e5a5a91075aa85ca6e\n",
      "  Stored in directory: c:\\users\\guillaume\\appdata\\local\\pip\\cache\\wheels\\91\\d0\\51\\088e48e1073c0efff046183c48761e6e3829c82e95cabaa392\n",
      "Successfully built axial_positional_embedding\n",
      "Installing collected packages: axial_positional_embedding\n",
      "Successfully installed axial_positional_embedding-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install axial_positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\Users\\\\Guillaume\\\\ViT_vs_Performers\\\\src\\\\Performers')\n",
    "from performers_pytorch import PerformerLM\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillaume\\AppData\\Local\\Temp\\ipykernel_20864\\2895174009.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Guillaume\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "training:   0%|          | 0/100000 [00:00<?, ?it/s]C:\\Users\\Guillaume\\AppData\\Local\\Temp\\ipykernel_20864\\2895174009.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\Guillaume\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "training:   0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GRADIENT_ACCUMULATE_EVERY):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 84\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28mnext\u001b[39m(\u001b[43mtrain_loader\u001b[49m), return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 2048\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "model = PerformerLM(\n",
    "    num_tokens = 256,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 8,\n",
    "    causal = True,\n",
    "    reversible = True,\n",
    "    nb_features = 256,\n",
    "    use_scalenorm = True,\n",
    "    shift_tokens = True,\n",
    "    local_attn_heads = (8, 8, 8, 6, 4, 2)\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "## model.cuda()\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "#with gzip.open('./data/enwik8.gz') as file:\n",
    "#    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
    "#    trX, vaX = np.split(X, [int(90e6)])\n",
    "#    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq #.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "#train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "#val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "#train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "#val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        with autocast():\n",
    "            loss = model(next(train_loader), return_loss = True)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "\n",
    "    scaler.unscale_(optim)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0 and i != 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
